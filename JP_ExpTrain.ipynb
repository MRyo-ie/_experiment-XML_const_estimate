{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python384jvsc74a57bd0d89399c061695a917c52cad24deccdb0a4139cdab3cdd06d8ca08ac226240f07",
   "display_name": "Python 3.8.4 64-bit ('3.8.4': pyenv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "2570507ff5ce8377770f843f2fd95b1e7af5e3bf65b540daf9dcf3f3f6ec669e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "from utils.Logger import showPlot\n",
    "from utils.Timer import asMinutes, timeSince\n",
    "\n",
    "%load_ext autoreload\n",
    "from data.exampleJP_Data import Lang, prepareData\n",
    "from model.rnn_model.encoderRNN import EncoderGRU, EncoderLSTM\n",
    "from model.rnn_model.decoderRNN import (\n",
    "    DecoderGRU, DecoderLSTM, \n",
    "    AttnDecoderGRU, AttnDecoderLSTM1, AttnDecoderLSTM2\n",
    ")\n",
    "from model.seq2seq_Model import (\n",
    "    Seq2Seq_batch_ptModel, \n",
    "    Seq2SeqTranslate_ptTokenizer,\n",
    ")\n",
    "from baselineJP_ExpTrain import example_ExpTrain\n",
    "from baseline_ExpEval import evaluate_batch_randomly\n",
    "%autoreload\n",
    "\n",
    "\n",
    "DATA_DIR = '_data_example'"
   ]
  },
  {
   "source": [
    "## load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: _data_example: File exists\n",
      "/Volumes/GoogleDrive/マイドライブ/__datasets__/[CV][NLP]「センター試験xml」/annotate_img/datas/center_exam_2011/construct/Experiment/_data_example\n",
      "fatal: destination path 'small_parallel_enja' already exists and is not an empty directory.\n",
      "------------------\n",
      "i can 't tell who will arrive first .\n",
      "many animals have been destroyed by men .\n",
      "i 'm in the tennis club .\n",
      "emi looks happy .\n",
      "please bear this fact in mind .\n",
      "誰 が 一番 に 着 く か 私 に は 分か り ま せ ん 。\n",
      "多く の 動物 が 人間 に よ っ て 滅ぼ さ れ た 。\n",
      "私 は テニス 部員 で す 。\n",
      "エミ は 幸せ そう に 見え ま す 。\n",
      "この 事実 を 心 に 留め て お い て 下さ い 。\n",
      "------------------\n",
      "\u001b[36mdata\u001b[m\u001b[m                data.zip            \u001b[36msmall_parallel_enja\u001b[m\u001b[m\n",
      "/Volumes/GoogleDrive/マイドライブ/__datasets__/[CV][NLP]「センター試験xml」/annotate_img/datas/center_exam_2011/construct/Experiment\n"
     ]
    }
   ],
   "source": [
    "if True:    # 解凍する場合は True に書き換え\n",
    "    !mkdir  $DATA_DIR\n",
    "    %cd  $DATA_DIR\n",
    "\n",
    "    !git clone https://github.com/odashi/small_parallel_enja\n",
    "    !echo '------------------'\n",
    "    !head -n 5 small_parallel_enja/train.en\n",
    "    !head -n 5 small_parallel_enja/train.ja\n",
    "    !echo '------------------'\n",
    "\n",
    "    !ls\n",
    "    %cd ../"
   ]
  },
  {
   "source": [
    "## setup Experiment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "source": [
    "## setup Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading lines...\n",
      "Read 50000 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "en 6637\n",
      "ja 8777\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(\n",
    "    'en', 'ja', f'{DATA_DIR}/small_parallel_enja', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['do you collect anything ?', '何 か 集め て い ま す か 。']\n6637 8777\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))\n",
    "print(input_lang.n_words, output_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_pairs, test_pairs = train_test_split(pairs, test_size=0.2)"
   ]
  },
  {
   "source": [
    "## setup Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "tokenizer = Seq2SeqTranslate_ptTokenizer(\n",
    "                    input_lang, output_lang, device)"
   ]
  },
  {
   "source": [
    "### test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Info] input_batch.shape, input_lens.shape\n     =  torch.Size([9, 10]) torch.Size([10])\n[Info] enc_outputs.shape, hidden_h.shape, hidden_c.shape\n     =  torch.Size([9, 10, 24]) torch.Size([1, 10, 12]) torch.Size([1, 10, 12])\n[Info] hidden[0].shape, hidden[1].shape\n     =  torch.Size([10, 12]) torch.Size([10, 12])\n[Info] dec_outputs.shape, hidden[0].shape, hidden[1].shape, attn_weights.shape\n     =  torch.Size([10, 8777]) torch.Size([10, 12]) torch.Size([10, 12]) torch.Size([10, 18])\n[Info] loss.item()\n     =  8.987950325012207\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "emb_size = 8\n",
    "hid_size = 12\n",
    "MAX_LENGTH = 18\n",
    "\n",
    "test_encoder = EncoderLSTM(input_lang.n_words, emb_size, hid_size)\n",
    "test_decoder1 = AttnDecoderLSTM1(\n",
    "                    emb_size, hid_size, output_lang.n_words,\n",
    "                    device, max_length=MAX_LENGTH)\n",
    "seq2seq_test_model = Seq2Seq_batch_ptModel(\n",
    "                    tokenizer, device,\n",
    "                    dropout_p=0.1, max_length=MAX_LENGTH)\n",
    "seq2seq_test_model.set_models(test_encoder, test_decoder1)\n",
    "seq2seq_test_model.exec_test(train_pairs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Info] input_batch.shape, input_lens.shape\n     =  torch.Size([17, 10]) torch.Size([10])\n[Info] enc_outputs.shape, hidden_h.shape, hidden_c.shape\n     =  torch.Size([17, 10, 24]) torch.Size([1, 10, 12]) torch.Size([1, 10, 12])\n[Info] hidden[0].shape, hidden[1].shape\n     =  torch.Size([10, 12]) torch.Size([10, 12])\n[Info] dec_outputs.shape, hidden[0].shape, hidden[1].shape, attn_weights.shape\n     =  torch.Size([10, 8777]) torch.Size([10, 12]) torch.Size([10, 12]) torch.Size([10, 17])\n[Info] loss.item()\n     =  9.162572860717773\n"
     ]
    }
   ],
   "source": [
    "attn_size = 9\n",
    "\n",
    "test_decoder2 = AttnDecoderLSTM2(\n",
    "                    emb_size, hid_size, attn_size, \n",
    "                    output_lang.n_words, device).to(device)\n",
    "seq2seq_test_model.set_models(test_encoder, test_decoder2)\n",
    "seq2seq_test_model.exec_test(train_pairs, batch_size=batch_size)"
   ]
  },
  {
   "source": [
    "## setup Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "from model.rnn_model.decoderRNN import (\n",
    "    DecoderGRU, DecoderLSTM, \n",
    "    AttnDecoderGRU, AttnDecoderLSTM1, AttnDecoderLSTM2\n",
    ")\n",
    "from model.seq2seq_Model import (\n",
    "    Seq2Seq_batch_ptModel, \n",
    "    Seq2SeqTranslate_ptTokenizer,\n",
    ")\n",
    "from baselineJP_ExpTrain import example_ExpTrain\n",
    "from baselineJP_ExpTrain import example_ExpTrain\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 1024\n",
    "hidden_size = 1024\n",
    "load_model_dir = '_best_weight_CPU'\n",
    "\n",
    "def rebuild_encoder():\n",
    "    encoder = EncoderLSTM(input_lang.n_words, emb_size, hidden_size)\n",
    "    # encoder.load_weights(\n",
    "    #             load_m_dir=f'_logs/{load_model_dir}', \n",
    "    #             load_m_file_name='encoder.pth')\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment\n",
    "exp_train = example_ExpTrain(train_pairs, test_pairs)"
   ]
  },
  {
   "source": [
    "## exec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3f08024f49f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                         save_m_file_names=(\n\u001b[1;32m     16\u001b[0m                             'encoder.pth', 'decoder_LSTM1.pth'))\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mseq2seq_lstm1_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m## exec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "encoder = rebuild_encoder()\n",
    "# decoder\n",
    "decoder1 = AttnDecoderLSTM1(\n",
    "                emb_size, hidden_size, output_lang.n_words, device)\n",
    "# decoder1.load_weights(\n",
    "#             load_m_dir=f'_logs/{load_model_dir}', \n",
    "#             load_m_file_name='decoder_LSTM1.pth')\n",
    "\n",
    "# seq2seq model\n",
    "seq2seq_lstm1_model = Seq2Seq_batch_ptModel(\n",
    "                        tokenizer, device,\n",
    "                        dropout_p=0.1, max_length=MAX_LENGTH,\n",
    "                        save_m_dir='_logs', \n",
    "                        save_m_file_names=(\n",
    "                            'encoder.pth', 'decoder_LSTM1.pth'))\n",
    "seq2seq_lstm1_model.set_models(encoder, decoder1)\n",
    "\n",
    "## exec\n",
    "exp_train.exec(seq2seq_lstm1_model, \n",
    "                epochs=3, batch_size=300, \n",
    "                teacher_forcing=0.9, early_stopping=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder = rebuild_encoder()\n",
    "# decoder\n",
    "decoder2 = AttnDecoderLSTM2(\n",
    "                emb_size, hidden_size, output_lang.n_words, device)\n",
    "# decoder2.load_weights(\n",
    "#             load_m_dir=f'_logs/{load_model_dir}', \n",
    "#             load_m_file_name='decoder_LSTM2.pth')\n",
    "\n",
    "# seq2seq model\n",
    "seq2seq_lstm2_model = Seq2Seq_batch_ptModel(\n",
    "                        tokenizer, device,\n",
    "                        dropout_p=0.1, max_length=MAX_LENGTH,\n",
    "                        save_m_dir='_logs', \n",
    "                        save_m_file_names=(\n",
    "                            'encoder.pth', 'decoder_LSTM2.pth'))\n",
    "seq2seq_lstm1_model.set_models(encoder, decoder1)\n",
    "\n",
    "## exec\n",
    "exp_train.exec(seq2seq_lstm2_model, \n",
    "                epochs=3, batch_size=300, \n",
    "                teacher_forcing=0.9, early_stopping=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_batch_randomly(seq2seq_lstm2_model, test_pairs, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}