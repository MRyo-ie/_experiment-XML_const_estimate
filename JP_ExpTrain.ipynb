{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python384jvsc74a57bd02570507ff5ce8377770f843f2fd95b1e7af5e3bf65b540daf9dcf3f3f6ec669e",
   "display_name": "Python 3.8.4 64-bit ('3.8.4': pyenv)",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "2570507ff5ce8377770f843f2fd95b1e7af5e3bf65b540daf9dcf3f3f6ec669e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "from utils.Logger import showPlot\n",
    "from utils.Timer import asMinutes, timeSince\n",
    "\n",
    "%load_ext autoreload\n",
    "from data.exampleJP_Data import Lang, prepareData\n",
    "from model.rnn_model.encoderRNN import EncoderGRU, EncoderLSTM\n",
    "from model.rnn_model.decoderRNN import (\n",
    "    DecoderGRU, DecoderLSTM, \n",
    "    AttnDecoderGRU, AttnDecoderLSTM1, AttnDecoderLSTM2\n",
    ")\n",
    "from model.seq2seq_Model import (\n",
    "    Seq2Seq_batch_ptModel, \n",
    "    Seq2SeqTranslate_ptTokenizer,\n",
    ")\n",
    "from baselineJP_ExpTrain import example_ExpTrain\n",
    "from baseline_ExpEval import evaluate_batch_randomly\n",
    "%autoreload\n",
    "\n",
    "\n",
    "DATA_DIR = '_data_example'"
   ]
  },
  {
   "source": [
    "## load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: _data_example: File exists\n",
      "/Volumes/GoogleDrive/マイドライブ/__datasets__/[CV][NLP]「センター試験xml」/annotate_img/datas/center_exam_2011/construct/Experiment/_data_example\n",
      "fatal: destination path 'small_parallel_enja' already exists and is not an empty directory.\n",
      "------------------\n",
      "i can 't tell who will arrive first .\n",
      "many animals have been destroyed by men .\n",
      "i 'm in the tennis club .\n",
      "emi looks happy .\n",
      "please bear this fact in mind .\n",
      "誰 が 一番 に 着 く か 私 に は 分か り ま せ ん 。\n",
      "多く の 動物 が 人間 に よ っ て 滅ぼ さ れ た 。\n",
      "私 は テニス 部員 で す 。\n",
      "エミ は 幸せ そう に 見え ま す 。\n",
      "この 事実 を 心 に 留め て お い て 下さ い 。\n",
      "------------------\n",
      "\u001b[36mdata\u001b[m\u001b[m                data.zip            \u001b[36msmall_parallel_enja\u001b[m\u001b[m\n",
      "/Volumes/GoogleDrive/マイドライブ/__datasets__/[CV][NLP]「センター試験xml」/annotate_img/datas/center_exam_2011/construct/Experiment\n"
     ]
    }
   ],
   "source": [
    "if True:    # 解凍する場合は True に書き換え\n",
    "    !mkdir  $DATA_DIR\n",
    "    %cd  $DATA_DIR\n",
    "\n",
    "    !git clone https://github.com/odashi/small_parallel_enja\n",
    "    !echo '------------------'\n",
    "    !head -n 5 small_parallel_enja/train.en\n",
    "    !head -n 5 small_parallel_enja/train.ja\n",
    "    !echo '------------------'\n",
    "\n",
    "    !ls\n",
    "    %cd ../"
   ]
  },
  {
   "source": [
    "## setup Experiment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "source": [
    "## setup Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading lines...\n",
      "Read 50000 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "en 6637\n",
      "ja 8777\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(\n",
    "    'en', 'ja', f'{DATA_DIR}/small_parallel_enja', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['he always wants to have his own way .', '彼 は いつ で も 自分 勝手 に や り たが る 。']\n6637 8777\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))\n",
    "print(input_lang.n_words, output_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_pairs, test_pairs = train_test_split(pairs, test_size=0.2)"
   ]
  },
  {
   "source": [
    "## setup Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "tokenizer = Seq2SeqTranslate_ptTokenizer(\n",
    "                    input_lang, output_lang, device)"
   ]
  },
  {
   "source": [
    "### test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Info] input_batch.shape, input_lens.shape\n",
      "     =  torch.Size([13, 10]) torch.Size([10])\n",
      "[Info] enc_outputs.shape, hidden_h.shape, hidden_c.shape\n",
      "     =  torch.Size([13, 10, 24]) torch.Size([1, 10, 12]) torch.Size([1, 10, 12])\n",
      "[Info] hidden[0].shape, hidden[1].shape\n",
      "     =  torch.Size([10, 12]) torch.Size([10, 12])\n",
      "[Info] dec_outputs.shape, hidden[0].shape, hidden[1].shape, attn_weights.shape\n",
      "     =  torch.Size([10, 8777]) torch.Size([10, 12]) torch.Size([10, 12]) torch.Size([10, 18])\n",
      "[Info] loss.item()\n",
      "     =  9.08955192565918\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "emb_size = 8\n",
    "hid_size = 12\n",
    "MAX_LENGTH = 18\n",
    "\n",
    "test_encoder = EncoderLSTM(input_lang.n_words, emb_size, hid_size)\n",
    "test_decoder1 = AttnDecoderLSTM1(\n",
    "                    emb_size, hid_size, output_lang.n_words,\n",
    "                    device, max_length=MAX_LENGTH)\n",
    "seq2seq_test_model = Seq2Seq_batch_ptModel(\n",
    "                    tokenizer, device,\n",
    "                    dropout_p=0.1, max_length=MAX_LENGTH)\n",
    "seq2seq_test_model.set_models(test_encoder, test_decoder1)\n",
    "seq2seq_test_model.exec_test(train_pairs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Info] input_batch.shape, input_lens.shape\n     =  torch.Size([13, 10]) torch.Size([10])\n[Info] enc_outputs.shape, hidden_h.shape, hidden_c.shape\n     =  torch.Size([13, 10, 24]) torch.Size([1, 10, 12]) torch.Size([1, 10, 12])\n[Info] hidden[0].shape, hidden[1].shape\n     =  torch.Size([10, 12]) torch.Size([10, 12])\n[Info] dec_outputs.shape, hidden[0].shape, hidden[1].shape, attn_weights.shape\n     =  torch.Size([10, 8777]) torch.Size([10, 12]) torch.Size([10, 12]) torch.Size([10, 13])\n[Info] loss.item()\n     =  9.074653625488281\n"
     ]
    }
   ],
   "source": [
    "attn_size = 9\n",
    "\n",
    "test_decoder2 = AttnDecoderLSTM2(\n",
    "                    emb_size, hid_size, attn_size, \n",
    "                    output_lang.n_words, device).to(device)\n",
    "seq2seq_test_model.set_models(test_encoder, test_decoder2)\n",
    "seq2seq_test_model.exec_test(train_pairs, batch_size=batch_size)"
   ]
  },
  {
   "source": [
    "## setup Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "from model.rnn_model.decoderRNN import (\n",
    "    DecoderGRU, DecoderLSTM, \n",
    "    AttnDecoderGRU, AttnDecoderLSTM1, AttnDecoderLSTM2\n",
    ")\n",
    "from model.seq2seq_Model import (\n",
    "    Seq2Seq_batch_ptModel, \n",
    "    Seq2SeqTranslate_ptTokenizer,\n",
    ")\n",
    "from baselineJP_ExpTrain import example_ExpTrain\n",
    "from baselineJP_ExpTrain import example_ExpTrain\n",
    "%autoreload\n",
    "\n",
    "load_model_dir = '_best_weight' if torch.cuda.is_available() else \"_best_weight_CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 1024\n",
    "hidden_size = 1024\n",
    "\n",
    "MAX_LENGTH = 18\n",
    "\n",
    "def build_encoder():\n",
    "    encoder = EncoderLSTM(input_lang.n_words, emb_size, hidden_size)\n",
    "    # encoder.load_weights(\n",
    "    #             load_m_dir=f'_logs/{load_model_dir}', \n",
    "    #             load_m_file_name='encoder.pth')\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment\n",
    "exp_train = example_ExpTrain(train_pairs, test_pairs)"
   ]
  },
  {
   "source": [
    "## exec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2it [00:29, 14.81s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0c95c1054c3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m## exec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m exp_train.exec(seq2seq_lstm1_model, \n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0;31m# epochs=3, batch_size=300,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/GoogleDrive/マイドライブ/__datasets__/[CV][NLP]「センター試験xml」/annotate_img/datas/center_exam_2011/construct/Experiment/baselineJP_ExpTrain.py\u001b[0m in \u001b[0;36mexec\u001b[0;34m(self, model, epochs, batch_size, teacher_forcing, early_stopping)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 loss = model.fit_batch(input_batch, input_lens,\n\u001b[0m\u001b[1;32m     66\u001b[0m                                         \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                                         optimizer, criterion, teacher_forcing)\n",
      "\u001b[0;32m/Volumes/GoogleDrive/マイドライブ/__datasets__/[CV][NLP]「センター試験xml」/annotate_img/datas/center_exam_2011/construct/Experiment/model/seq2seq_Model.py\u001b[0m in \u001b[0;36mfit_batch\u001b[0;34m(self, input_batch, input_lens, target_batch, target_lens, optimizer, criterion, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 dec_output, dec_hidden, attention = self.decoder(\n\u001b[0m\u001b[1;32m    170\u001b[0m                     dec_inputs[di], dec_hidden, enc_outputs)\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.4/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/GoogleDrive/マイドライブ/__datasets__/[CV][NLP]「センター試験xml」/annotate_img/datas/center_exam_2011/construct/Experiment/model/rnn_model/decoderRNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (b,h),((b,h),(b,h)) -> (b,h)((b,h),(b,h))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (b,h) -> (b,o)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m  \u001b[0;31m# (b,o),(b,h),(b,il)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.4/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.4/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.4/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "encoder = build_encoder()\n",
    "# decoder\n",
    "decoder1 = AttnDecoderLSTM1(\n",
    "                emb_size, hidden_size, output_lang.n_words, device)\n",
    "# decoder1.load_weights(\n",
    "#             load_m_dir=f'_logs/{load_model_dir}', \n",
    "#             load_m_file_name='decoder_LSTM1.pth')\n",
    "\n",
    "# seq2seq model\n",
    "seq2seq_lstm1_model = Seq2Seq_batch_ptModel(\n",
    "                        tokenizer, device,\n",
    "                        dropout_p=0.1, max_length=MAX_LENGTH,\n",
    "                        save_m_dir='_logs', \n",
    "                        save_m_file_names=(\n",
    "                            'encoder.pth', 'decoder_LSTM1.pth'))\n",
    "seq2seq_lstm1_model.set_models(encoder, decoder1)\n",
    "\n",
    "## exec\n",
    "exp_train.exec(seq2seq_lstm1_model, \n",
    "                # epochs=3, batch_size=300, \n",
    "                epochs=20, batch_size=200, \n",
    "                teacher_forcing=0.9, early_stopping=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder = build_encoder()\n",
    "# decoder\n",
    "attn_size = 1024\n",
    "decoder2 = AttnDecoderLSTM2(\n",
    "                emb_size, hidden_size, attn_size, \n",
    "                output_lang.n_words, device)\n",
    "# decoder2.load_weights(\n",
    "#             load_m_dir=f'_logs/{load_model_dir}', \n",
    "#             load_m_file_name='decoder_LSTM2.pth')\n",
    "\n",
    "# seq2seq model\n",
    "seq2seq_lstm2_model = Seq2Seq_batch_ptModel(\n",
    "                        tokenizer, device,\n",
    "                        dropout_p=0.1, max_length=MAX_LENGTH,\n",
    "                        save_m_dir='_logs', \n",
    "                        save_m_file_names=(\n",
    "                            'encoder.pth', 'decoder_LSTM2.pth'))\n",
    "seq2seq_lstm2_model.set_models(encoder, decoder2)\n",
    "\n",
    "## exec\n",
    "exp_train.exec(seq2seq_lstm2_model, \n",
    "                # epochs=3, batch_size=300, \n",
    "                epochs=20, batch_size=200, \n",
    "                teacher_forcing=0.9, early_stopping=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_batch_randomly(seq2seq_lstm2_model, test_pairs, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}